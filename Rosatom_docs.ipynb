{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ввод датасета/нашей документации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from striprtf.striprtf import rtf_to_text\n",
    "import os\n",
    "from nltk.tokenize import LineTokenizer\n",
    "import pandas as pd\n",
    "#import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPath=\"./docs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltokenizer=LineTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rtf_names(loc_path):\n",
    "    rtf_names=[]\n",
    "    names=os.listdir(path=loc_path)\n",
    "    for name in names:\n",
    "        if \".rtf\" in name:\n",
    "            rtf_names.append(loc_path+name)\n",
    "    return rtf_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_doc(fname):\n",
    "    file=open(fname, 'r')\n",
    "    rtfdoc=file.read()\n",
    "    document=rtf_to_text(rtfdoc)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hd(document, tokenizer=ltokenizer):\n",
    "    lines=tokenizer.tokenize(document)\n",
    "    tok_head=\"Должностная инструкция\"\n",
    "    i_head=0\n",
    "    cnt=0\n",
    "    for l in lines:\n",
    "        if tok_head in l:\n",
    "            i_head=cnt\n",
    "            break\n",
    "        cnt+=1\n",
    "    hline=lines[i_head]\n",
    "    if i_head<len(lines)-1:\n",
    "        hline+=lines[i_head+1]\n",
    "    return hline, lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_text_pd(paragraph_breaker=ltokenizer):\n",
    "    rtf_names=get_rtf_names(myPath)\n",
    "    #rtf_names=[\"./docs/005.rtf\", \"./docs/006.rtf\"]\n",
    "    df=pd.DataFrame(index=range(0,len(rtf_names)), columns=[\"Position\", \"Text\",\"File name\"])\n",
    "    cnt=0\n",
    "    for name in rtf_names:\n",
    "        document=get_text_doc(name)\n",
    "        hline, lines=parse_hd(document,tokenizer=paragraph_breaker)\n",
    "        df.iloc[cnt, 0]=hline\n",
    "        df.iloc[cnt, 1]=lines\n",
    "        df.iloc[cnt, 2]=name\n",
    "        cnt+=1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=get_data_text_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_json(data,local_path=\"./docs/\"):\n",
    "    short_data=pd.DataFrame(index=all_data.index, columns=[\"id\", \"Position\", \"path\"])\n",
    "    short_data[\"id\"]=all_data.index.copy(deep=True)\n",
    "    short_data[\"Position\"]=all_data.Position.copy(deep=True)\n",
    "    short_data[\"path\"]=all_data[\"File name\"].copy(deep=True)\n",
    "    short_data.to_json(path_or_buf=local_path+\"gen_table.json\", orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_json(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(init_text, tokenizer, stop_words, morpher):\n",
    "    init_text=init_text.lower()\n",
    "    tokens=tokenizer.tokenize(init_text)\n",
    "    tokens_clear=[]\n",
    "    for t in tokens:\n",
    "        if t in stop_words:\n",
    "            continue\n",
    "        tokens_clear.append(t)\n",
    "    tokens_lemmatized=[]\n",
    "    for t in tokens_clear:\n",
    "        p = morph.parse(t)[0]\n",
    "        p=p.normal_form\n",
    "        tokens_lemmatized.append(p)\n",
    "    return tokens_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Готовим инструменты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import pymorphy2\n",
    "\n",
    "#from nltk import pos_tag_sents\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "#nltk_download('averaged_perceptron_tagger_ru')\n",
    "#nltk.download('universal_tagset')\n",
    "#pos_tag_sents(word_tokenize(\"настоящей должностной инструкцией\"), tagset='universal',lang='rus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подключаем gensim и загружаем векторную модель слов (уже готовую, model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.BASE_DIR='.\\\\docs\\\\model\\\\' #myPath+'model/' #C:\\\\Users\\\\Home\\\\gensim-data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\docs\\\\model\\\\'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=api.load(\"word2vec-ruscorpora-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_path=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_path=os.getcwd()+'\\\\docs\\\\model\\\\model.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Home\\\\dl-rosatom\\\\docs\\\\model\\\\model.model'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 03:53:55: loading Word2VecKeyedVectors object from C:\\Users\\Home\\dl-rosatom\\docs\\model\\model.model\n",
      "INFO - 03:53:56: loading vectors from C:\\Users\\Home\\dl-rosatom\\docs\\model\\model.model.vectors.npy with mmap=None\n",
      "INFO - 03:53:56: loading vectors_ngrams from C:\\Users\\Home\\dl-rosatom\\docs\\model\\model.model.vectors_ngrams.npy with mmap=None\n",
      "INFO - 03:53:57: loading vectors_vocab from C:\\Users\\Home\\dl-rosatom\\docs\\model\\model.model.vectors_vocab.npy with mmap=None\n",
      "INFO - 03:53:57: setting ignored attribute vectors_ngrams_norm to None\n",
      "INFO - 03:53:57: setting ignored attribute vectors_norm to None\n",
      "INFO - 03:53:57: setting ignored attribute buckets_word to None\n",
      "INFO - 03:53:57: setting ignored attribute vectors_vocab_norm to None\n",
      "INFO - 03:53:57: loaded C:\\Users\\Home\\dl-rosatom\\docs\\model\\model.model\n"
     ]
    }
   ],
   "source": [
    "md=gensim.models.KeyedVectors.load(mod_path) #(\"C:\\\\Users\\\\Home\\\\gensim-data\\\\model.model\") #C:\\\\Users\\\\Home\\\\gensim-data\\\\model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получаем входной текст и предобрабатываем его: леммтизация, токенизация, выкидывание стоп-слов, частых/редких слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sys.argv)>3:\n",
    "    hdr=sys.argv[1]\n",
    "    init_text=hdr+sys.argv[2]+sys.argv[3]\n",
    "else:\n",
    "    hdr=\"Электрик\"\n",
    "    init_text=\"службы начальника производства испытания устройств и электротехнических измерений монтаж сетей электрообородования\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdr=\"Генеральный директор\"\n",
    "#init_text=\"Администрация, руководство персоналом, работа с документами\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиваем текст на отдельные токены (здесь, по сути, на слова)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+\\.') # с паттернами сложно, надо отрегулировать #'\\w+|[^\\w\\s]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 03:53:57: Loading dictionaries from C:\\local\\Anaconda3\\lib\\site-packages\\pymorphy2_dicts\\data\n",
      "INFO - 03:53:57: format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lemmatized=prepare_text(init_text, tokenizer, stop_words, morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получаем входную базу текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#headers, data_text, the_rest=get_data_text()\n",
    "# pd.DataFrame - all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получаем рекомендуемый текст и заголовки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сначала - топ рекомендуемых документов, выбранных по заголовкам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_token(lst,token):\n",
    "    n=lst.count(token)\n",
    "    if n>0:\n",
    "        lst.remove(token)\n",
    "        rem_token(lst,token)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_docs(hdr, data, REtokenizer, stop_words, morpher, model, n=5):\n",
    "    hdr_lemmatized=prepare_text(hdr, REtokenizer, stop_words, morpher)\n",
    "    cnt=0\n",
    "    header_sim=[]\n",
    "    token1=\"должностной\"\n",
    "    token2=\"инструкция\"\n",
    "    for i in range(0,data.shape[0]):\n",
    "        position=data.loc[i,\"Position\"]\n",
    "        pos_lemmatized=prepare_text(position, REtokenizer, stop_words, morpher)\n",
    "        rem_token(pos_lemmatized,token1)\n",
    "        rem_token(pos_lemmatized,token2)\n",
    "        hsim=model.n_similarity(hdr_lemmatized, pos_lemmatized)\n",
    "        header_sim.append((hsim,i))\n",
    "    header_sim.sort(reverse=True)\n",
    "    top_docs=[]\n",
    "    if n>len(header_sim):\n",
    "        n=len(header_sim)\n",
    "    for i in range(0,n):\n",
    "        j=header_sim[i][1]\n",
    "        top_docs.append(j)\n",
    "    return top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_texts(init_text, data, top_docs, REtokenizer, stop_words, morpher, model, n=5):\n",
    "    tokens_lemmatized=prepare_text(init_text, REtokenizer, stop_words, morpher)\n",
    "    sim_list=[]\n",
    "    result=pd.DataFrame(index=range(0,n), columns=[\"Position\", \"Text\", \"File name\"])\n",
    "    result[\"Text\"]=result.Text.astype('str')\n",
    "    for i in top_docs:\n",
    "        doc_text=data.loc[i,\"Text\"]\n",
    "        cnt=0\n",
    "        for text in doc_text:\n",
    "            text_lemmatized=prepare_text(text, REtokenizer, stop_words, morpher)\n",
    "            if (len(tokens_lemmatized)==0 or len(text_lemmatized)==0):\n",
    "                continue\n",
    "            sim=model.n_similarity(tokens_lemmatized, text_lemmatized)\n",
    "            sim_list.append((sim,cnt,i))\n",
    "            cnt+=1\n",
    "    sim_list.sort(reverse=True)\n",
    "    if n>len(sim_list):\n",
    "        n=len(sim_list)\n",
    "    for i in range(0,n):\n",
    "        ind=sim_list[i][2]\n",
    "        text_ind=sim_list[i][1]\n",
    "        result.iloc[i,0]=data.loc[ind,\"Position\"]\n",
    "        result.iloc[i,1]+=(data.loc[ind,\"Text\"][text_ind]+'\\n')\n",
    "        result.iloc[i,2]=data.loc[ind,\"File name\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_docs=get_top_docs(hdr, all_data, tokenizer, stop_words, morph, md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=get_top_texts(init_text, all_data, top_docs, tokenizer, stop_words, morph, md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_data=pd.DataFrame(index=result.index, columns=[\"id\", \"Position\", \"Text\", \"path\"])\n",
    "dump_data[\"id\"]=result.index.copy(deep=True)\n",
    "dump_data[\"Position\"]=result.Position.copy(deep=True)\n",
    "dump_data[\"path\"]=result[\"File name\"].copy(deep=True)\n",
    "dump_data[\"Text\"]=result[\"Text\"].copy(deep=True)\n",
    "dump_data.to_json(path_or_buf=\"./docs/\"+\"result_table.json\", orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nan 4.2. Перспективы технического развития предприятия.\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.loc[4,\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
