{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ввод датасета/нашей документации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from striprtf.striprtf import rtf_to_text\n",
    "import os\n",
    "from nltk.tokenize import LineTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltokenizer=LineTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rtf_names():\n",
    "    rtf_names=[]\n",
    "    names=os.listdir()\n",
    "    for name in names:\n",
    "        if \".rtf\" in name:\n",
    "            rtf_names.append(name)\n",
    "    return rtf_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_doc(fname):\n",
    "    file=open(fname, 'r')\n",
    "    rtfdoc=file.read()\n",
    "    document=rtf_to_text(rtfdoc)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_instruction(document, tokenizer=ltokenizer):\n",
    "    lines=tokenizer.tokenize(document)\n",
    "    tok_begin=\"Должностные обязанности\"\n",
    "    tok_end=\"Права\"\n",
    "    i_begin=0\n",
    "    i_end=0\n",
    "    cnt=0\n",
    "    for l in lines:\n",
    "        if tok_begin in l:\n",
    "            i_begin=cnt+1\n",
    "        if tok_end in l:\n",
    "            i_end=cnt\n",
    "        cnt+=1\n",
    "    hline=lines[i_begin]\n",
    "    main_cont=lines[i_begin+1:i_end]\n",
    "    the_rest=lines[0:i_begin]+lines[i_end+1:-1]\n",
    "    return hline, main_cont, the_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_text(paragraph_breaker=ltokenizer):\n",
    "    headers=[]\n",
    "    data_text=[]\n",
    "    secondary_text=[]\n",
    "    rtf_names=get_rtf_names()\n",
    "    for name in rtf_names:\n",
    "        document=get_text_doc(name)\n",
    "        hline, main_cont, the_rest=parse_instruction(document,tokenizer=paragraph_breaker)\n",
    "        for k in range(0,len(main_cont)):\n",
    "            headers.append(hline)\n",
    "        data_text+=main_cont\n",
    "        secondary_text+=the_rest\n",
    "    return headers, data_text, secondary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(init_text, tokenizer, stop_words, morpher):\n",
    "    init_text=init_text.lower()\n",
    "    tokens=tokenizer.tokenize(init_text)\n",
    "    tokens_clear=[]\n",
    "    for t in tokens:\n",
    "        if t in stop_words:\n",
    "            continue\n",
    "        tokens_clear.append(t)\n",
    "    tokens_lemmatized=[]\n",
    "    for t in tokens_clear:\n",
    "        p = morph.parse(t)[0]\n",
    "        p=p.normal_form\n",
    "        tokens_lemmatized.append(p)\n",
    "    return tokens_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_docs(init_text, headers, data_text, REtokenizer, stop_words, morpher, model, n=5):\n",
    "    tokens_lemmatized=prepare_text(init_text, REtokenizer, stop_words, morpher)\n",
    "    sim_ind=[]\n",
    "    cnt=0\n",
    "    for text in data_text:\n",
    "        doc=prepare_text(text,REtokenizer, stop_words, morpher)\n",
    "        if (len(doc)==0 or len(tokens_lemmatized)==0):\n",
    "            continue\n",
    "        sim=model.n_similarity(tokens_lemmatized, doc)\n",
    "        sim_ind.append((sim,cnt))\n",
    "        cnt+=1\n",
    "    sim_ind.sort(reverse=True)\n",
    "    top_docs=[]\n",
    "    top_headers=[]\n",
    "    if n>len(sim_ind):\n",
    "        return\n",
    "    for i in range(0,n):\n",
    "        j=sim_ind[i][1]\n",
    "        top_docs.append(data_text[j])\n",
    "        if headers[j] not in top_headers:\n",
    "            top_headers.append(headers[j])\n",
    "    return top_docs, headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Готовим инструменты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import pymorphy2\n",
    "\n",
    "#from nltk import pos_tag_sents\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "#nltk_download('averaged_perceptron_tagger_ru')\n",
    "#nltk.download('universal_tagset')\n",
    "#pos_tag_sents(word_tokenize(\"настоящей должностной инструкцией\"), tagset='universal',lang='rus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подключаем gensim и загружаем векторную модель слов (уже готовую, model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.BASE_DIR='C:\\\\Users\\\\Home\\\\gensim-data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Home\\\\gensim-data\\\\'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=api.load(\"word2vec-ruscorpora-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 20:25:55: loading Word2VecKeyedVectors object from C:\\Users\\Home\\gensim-data\\model.model\n",
      "INFO - 20:25:56: loading vectors from C:\\Users\\Home\\gensim-data\\model.model.vectors.npy with mmap=None\n",
      "INFO - 20:25:56: loading vectors_ngrams from C:\\Users\\Home\\gensim-data\\model.model.vectors_ngrams.npy with mmap=None\n",
      "INFO - 20:25:57: loading vectors_vocab from C:\\Users\\Home\\gensim-data\\model.model.vectors_vocab.npy with mmap=None\n",
      "INFO - 20:25:57: setting ignored attribute vectors_ngrams_norm to None\n",
      "INFO - 20:25:57: setting ignored attribute vectors_norm to None\n",
      "INFO - 20:25:57: setting ignored attribute buckets_word to None\n",
      "INFO - 20:25:57: setting ignored attribute vectors_vocab_norm to None\n",
      "INFO - 20:25:57: loaded C:\\Users\\Home\\gensim-data\\model.model\n"
     ]
    }
   ],
   "source": [
    "md=gensim.models.KeyedVectors.load(\"C:\\\\Users\\\\Home\\\\gensim-data\\\\model.model\") #C:\\\\Users\\\\Home\\\\gensim-data\\\\model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получаем входной текст и предобрабатываем его: леммтизация, токенизация, выкидывание стоп-слов, частых/редких слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_text=\"Генеральный директор, руководство персоналом, работа с документами\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиваем текст на отдельные токены (здесь, по сути, на слова)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+\\.') # с паттернами сложно, надо отрегулировать #'\\w+|[^\\w\\s]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 20:25:57: Loading dictionaries from C:\\local\\Anaconda3\\lib\\site-packages\\pymorphy2_dicts\\data\n",
      "INFO - 20:25:57: format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lemmatized=prepare_text(init_text, tokenizer, stop_words, morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получаем входную базу текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers, data_text, the_rest=get_data_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получаем рекомендуемый текст и заголовки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_docs, top_hdr=get_top_docs(init_text, headers, data_text, tokenizer, stop_words,morph, md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 1. Осуществляет руководство работой персонала машинописного бюро.',\n",
       " ' 1.2. Передача докладных записок после их регистрации заведующему канцелярией (начальнику отдела документационного обеспечения) для доклада руководству организации.',\n",
       " '2. Принимает поступающую на рассмотрение руководителя корреспонденцию, передает ее в соответствии с принятым решением в структурные подразделения или конкретным исполнителем для использования в процессе работы либо подготовки ответов.',\n",
       " '3. Руководит работой по подготовке необходимых отчетов и справок, связанных с деятельностью бюро и выдачей пропускных документов.',\n",
       " 'Оказывать помощь коллегам по работе при решении задач их деятельности в случае, если помощь может привести к качественному улучшению результатов деятельности. ',\n",
       " 'Участвовать в реализации мероприятий по формированию и транслировании корпоративной культуры и улучшению социально-психологического климата в коллективе. ',\n",
       " '4. Планирует внеклассную и внешкольную воспитательную работу, обеспечивает и контролирует ее выполнение.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_hdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
